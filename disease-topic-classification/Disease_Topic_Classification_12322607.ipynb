{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Disease topic classification with LLMs.\n",
    "\n",
    "- Loads few-shot examples and test notes from 'data-note-with-diseases-label.xlsx'\n",
    "- Runs GPT-4.1-mini (A), GPT-4.1 (B), and LLaMA 3 8B (C)\n",
    "- For each model: four prompt variants (v0-v3)\n",
    "- Writes predictions to 'tests_with_predictions_all_models.csv'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62449cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai scikit-learn huggingface_hub pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# config / setup \n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "llama_client = InferenceClient(\n",
    "    api_key=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct:novita\"\n",
    "\n",
    "file_path = \"data-note-with-diseases-label.xlsx\"\n",
    "few = pd.read_excel(file_path, sheet_name=\"few-shot examples\")\n",
    "tests = pd.read_excel(file_path, sheet_name=\"tests-with-the-ground-truth\")\n",
    "\n",
    "label_set = sorted(\n",
    "    lab.strip()\n",
    "    for s in few[\"diseases_label\"]\n",
    "    if isinstance(s, str)\n",
    "    for lab in s.split(\",\")\n",
    ")\n",
    "print(\"Labels:\", label_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc255ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "\n",
    "def call_openai(prompt, model=\"gpt-4.1-mini\", max_retries=3, sleep_sec=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = openai_client.responses.create(\n",
    "                model=model,\n",
    "                input=prompt,\n",
    "                max_output_tokens=128,\n",
    "            )\n",
    "            parts = resp.output[0].content\n",
    "            text = \"\".join(p.text for p in parts if hasattr(p, \"text\"))\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[{model}] error {attempt+1}/{max_retries}: {e}\")\n",
    "            time.sleep(sleep_sec)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def call_llama(prompt, max_tokens=128):\n",
    "    try:\n",
    "        completion = llama_client.chat.completions.create(\n",
    "            model=LLAMA_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        # HF OpenAI-compatible API: message is dict-like\n",
    "        msg = completion.choices[0].message\n",
    "        if isinstance(msg, dict):\n",
    "            return msg.get(\"content\", \"\").strip()\n",
    "        # fallback if it's an object\n",
    "        return getattr(msg, \"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[llama] error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_labels(raw_text, label_list):\n",
    "    if not raw_text:\n",
    "        return []\n",
    "\n",
    "    text = raw_text.strip()\n",
    "    if text.lower().startswith(\"labels:\"):\n",
    "        text = text[len(\"labels:\"):].strip()\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\",\") if p.strip()]\n",
    "    allowed = {lab.lower(): lab for lab in label_list}\n",
    "\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        lab = allowed.get(p.lower())\n",
    "        if lab is not None:\n",
    "            out.append(lab)\n",
    "\n",
    "    # deduplicate, keep order\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "\n",
    "def parse_labels_v3(raw_text, label_list):\n",
    "    if not raw_text:\n",
    "        return []\n",
    "\n",
    "    lines = [ln.strip() for ln in raw_text.splitlines() if ln.strip()]\n",
    "    label_line = None\n",
    "    for ln in reversed(lines):\n",
    "        if ln.lower().startswith(\"labels:\"):\n",
    "            label_line = ln\n",
    "            break\n",
    "\n",
    "    if label_line is None:\n",
    "        return parse_labels(raw_text, label_list)\n",
    "\n",
    "    after = label_line[len(\"labels:\"):].strip()\n",
    "    return parse_labels(after, label_list)\n",
    "\n",
    "\n",
    "label_definitions = {\n",
    "    \"periodontitis\": \"inflammatory disease affecting the supporting tissues of the teeth\",\n",
    "    \"cerebral aneurysms\": \"focal dilatations of intracranial arteries in the brain\",\n",
    "    \"posterior parietal neoplasm\": \"tumour located in the posterior parietal region of the brain\",\n",
    "    \"glioblastoma multiforme\": \"aggressive high-grade primary brain tumour\",\n",
    "    \"status epilepticus\": \"seizure activity lasting more than 5 minutes or repeated seizures without recovery\",\n",
    "    \"hemiparesis\": \"weakness of one side of the body (left or right)\",\n",
    "    \"pneumonia\": \"infection of the lung parenchyma with cough, fever or respiratory findings\",\n",
    "    \"sepsis\": \"life-threatening organ dysfunction caused by a dysregulated host response to infection\",\n",
    "    \"shortness of breath\": \"dyspnoea or difficulty breathing\",\n",
    "    \"cough\": \"expulsive reflex to clear airways, often associated with respiratory disease\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd955b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt builders\n",
    "\n",
    "def build_prompt_v0(note_text, label_list):\n",
    "    return f\"\"\"\n",
    "You are a medical expert.\n",
    "\n",
    "Task:\n",
    "Read the following patient note and assign one or more disease labels from the allowed list.\n",
    "\n",
    "Allowed disease labels (use only these, separated by commas):\n",
    "{\", \".join(label_list)}\n",
    "\n",
    "Rules:\n",
    "- Use only labels from the allowed list.\n",
    "- Output must be a comma-separated list of labels, with no extra text.\n",
    "- If more than one disease is present, include all relevant labels.\n",
    "- If none clearly apply, choose the single most likely label instead of writing \"none\".\n",
    "\n",
    "Patient note:\n",
    "{note_text.strip()}\n",
    "\n",
    "Labels:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_fewshot_prompt_v1(few_df, note_text, label_list):\n",
    "    examples = []\n",
    "    for i, row in few_df.iterrows():\n",
    "        examples.append(\n",
    "            f\"Example {i+1}\\n\"\n",
    "            f\"Note: {row['description'].strip()}\\n\"\n",
    "            f\"Labels: {row['diseases_label']}\\n\"\n",
    "        )\n",
    "    examples_block = \"\\n\\n\".join(examples)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a medical expert.\n",
    "\n",
    "Task:\n",
    "Read a patient note and assign one or more disease labels from the allowed list.\n",
    "\n",
    "Allowed disease labels (use only these, separated by commas):\n",
    "{\", \".join(label_list)}\n",
    "\n",
    "Rules:\n",
    "- Use only labels from the allowed list.\n",
    "- Output must be a comma-separated list of labels, with no extra text.\n",
    "- If more than one disease is present, include all relevant labels.\n",
    "- If none clearly apply, choose the single most likely label.\n",
    "\n",
    "Here are labeled examples:\n",
    "\n",
    "{examples_block}\n",
    "\n",
    "Now classify this new patient note.\n",
    "\n",
    "Note: {note_text.strip()}\n",
    "Labels:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_fewshot_prompt_v2(few_df, note_text, label_list, label_defs):\n",
    "    defs_block = \"\\n\".join(\n",
    "        f\"- {lab}: {label_defs[lab]}\" for lab in label_list\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    for i, row in few_df.iterrows():\n",
    "        examples.append(\n",
    "            f\"Example {i+1}\\n\"\n",
    "            f\"Note: {row['description'].strip()}\\n\"\n",
    "            f\"Labels: {row['diseases_label']}\\n\"\n",
    "        )\n",
    "    examples_block = \"\\n\\n\".join(examples)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a medical expert.\n",
    "\n",
    "Task:\n",
    "Read a patient note and assign one or more disease labels from the allowed list.\n",
    "\n",
    "Allowed disease labels and brief definitions:\n",
    "{defs_block}\n",
    "\n",
    "Rules:\n",
    "- Use only labels from the allowed list.\n",
    "- ALWAYS output at least one label from the allowed list.\n",
    "- Output must be a comma-separated list of labels, with no extra text.\n",
    "- Do NOT output words like \"none\" or \"no disease\".\n",
    "- If more than one disease is present, include all relevant labels.\n",
    "- If uncertain, choose the single most likely label.\n",
    "\n",
    "Here are labeled examples:\n",
    "\n",
    "{examples_block}\n",
    "\n",
    "Now classify this new patient note.\n",
    "\n",
    "Note: {note_text.strip()}\n",
    "Labels:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_fewshot_prompt_v3(few_df, note_text, label_list, label_defs):\n",
    "    defs_block = \"\\n\".join(\n",
    "        f\"- {lab}: {label_defs[lab]}\" for lab in label_list\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    for i, row in few_df.iterrows():\n",
    "        examples.append(\n",
    "            f\"Example {i+1}\\n\"\n",
    "            f\"Note: {row['description'].strip()}\\n\"\n",
    "            f\"Reasoning: Briefly identify the key clinical features and which disease labels apply.\\n\"\n",
    "            f\"Labels: {row['diseases_label']}\\n\"\n",
    "        )\n",
    "    examples_block = \"\\n\\n\".join(examples)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a medical expert.\n",
    "\n",
    "Task:\n",
    "Read a patient note and assign one or more disease labels from the allowed list.\n",
    "\n",
    "Allowed disease labels and brief definitions:\n",
    "{defs_block}\n",
    "\n",
    "Instructions:\n",
    "- Think step by step about the key clinical findings in the note and which disease labels apply.\n",
    "- Use only labels from the allowed list.\n",
    "- ALWAYS output at least one label from the allowed list.\n",
    "- Do NOT output words like \"none\", \"no disease\", or free text diagnoses outside the label list.\n",
    "- In the final answer, strictly follow this format:\n",
    "\n",
    "Reasoning: <1-3 sentences explaining the key findings and the chosen labels>\n",
    "Labels: <comma-separated list of labels, with no extra text>\n",
    "\n",
    "Here are labeled examples:\n",
    "\n",
    "{examples_block}\n",
    "\n",
    "Now classify this new patient note.\n",
    "\n",
    "Note: {note_text.strip()}\n",
    "\n",
    "First provide your reasoning, then the final labels on a separate line as specified.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7add133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run models \n",
    "\n",
    "def run_openai_model(prompt_fn, parse_fn, base_model, col_name):\n",
    "    preds = []\n",
    "    arg_names = prompt_fn.__code__.co_varnames\n",
    "\n",
    "    for _, row in tests.iterrows():\n",
    "        note = row[\"description\"]\n",
    "\n",
    "        args = []\n",
    "        # if prompt_fn expects few_df -> pass it as first argument\n",
    "        if \"few_df\" in arg_names:\n",
    "            args.append(few)\n",
    "        # then always note_text, label_list\n",
    "        args.append(note)\n",
    "        args.append(label_set)\n",
    "        # if prompt_fn expects label_defs -> pass it as last argument\n",
    "        if \"label_defs\" in arg_names:\n",
    "            args.append(label_definitions)\n",
    "\n",
    "        prompt = prompt_fn(*args)\n",
    "        raw = call_openai(prompt, model=base_model)\n",
    "        preds.append(parse_fn(raw, label_set))\n",
    "\n",
    "    tests[col_name] = [\", \".join(p) for p in preds]\n",
    "\n",
    "\n",
    "def run_llama_model(prompt_fn, parse_fn, col_name):\n",
    "    preds = []\n",
    "    arg_names = prompt_fn.__code__.co_varnames\n",
    "\n",
    "    for _, row in tests.iterrows():\n",
    "        note = row[\"description\"]\n",
    "\n",
    "        args = []\n",
    "        if \"few_df\" in arg_names:\n",
    "            args.append(few)\n",
    "        args.append(note)\n",
    "        args.append(label_set)\n",
    "        if \"label_defs\" in arg_names:\n",
    "            args.append(label_definitions)\n",
    "\n",
    "        prompt = prompt_fn(*args)\n",
    "        raw = call_llama(prompt)\n",
    "        preds.append(parse_fn(raw, label_set))\n",
    "\n",
    "    tests[col_name] = [\", \".join(p) for p in preds]\n",
    "\n",
    "\n",
    "# A0–A3: gpt-4.1-mini\n",
    "run_openai_model(build_prompt_v0,         parse_labels,    \"gpt-4.1-mini\", \"pred_A0_mini\")\n",
    "run_openai_model(build_fewshot_prompt_v1, parse_labels,    \"gpt-4.1-mini\", \"pred_A1_mini\")\n",
    "run_openai_model(build_fewshot_prompt_v2, parse_labels,    \"gpt-4.1-mini\", \"pred_A2_mini\")\n",
    "run_openai_model(build_fewshot_prompt_v3, parse_labels_v3, \"gpt-4.1-mini\", \"pred_A3_mini\")\n",
    "\n",
    "# B0–B3: gpt-4.1\n",
    "run_openai_model(build_prompt_v0,         parse_labels,    \"gpt-4.1\",      \"pred_B0_gpt41\")\n",
    "run_openai_model(build_fewshot_prompt_v1, parse_labels,    \"gpt-4.1\",      \"pred_B1_gpt41\")\n",
    "run_openai_model(build_fewshot_prompt_v2, parse_labels,    \"gpt-4.1\",      \"pred_B2_gpt41\")\n",
    "run_openai_model(build_fewshot_prompt_v3, parse_labels_v3, \"gpt-4.1\",      \"pred_B3_gpt41\")\n",
    "\n",
    "# C0–C3: LLaMA\n",
    "run_llama_model(build_prompt_v0,         parse_labels,    \"pred_C0_llama\")\n",
    "run_llama_model(build_fewshot_prompt_v1, parse_labels,    \"pred_C1_llama\")\n",
    "run_llama_model(build_fewshot_prompt_v2, parse_labels,    \"pred_C2_llama\")\n",
    "run_llama_model(build_fewshot_prompt_v3, parse_labels_v3, \"pred_C3_llama\")\n",
    "\n",
    "\n",
    "# save / quick check \n",
    "\n",
    "print(\n",
    "    tests[[\n",
    "        \"_id\",\n",
    "        \"pred_A0_mini\",\n",
    "        \"pred_A1_mini\",\n",
    "        \"pred_A2_mini\",\n",
    "        \"pred_A3_mini\",\n",
    "        \"pred_B0_gpt41\",\n",
    "        \"pred_B1_gpt41\",\n",
    "        \"pred_B2_gpt41\",\n",
    "        \"pred_B3_gpt41\",\n",
    "        \"pred_C0_llama\",\n",
    "        \"pred_C1_llama\",\n",
    "        \"pred_C2_llama\",\n",
    "        \"pred_C3_llama\",\n",
    "    ]].head(10).to_string(index=False)\n",
    ")\n",
    "\n",
    "tests.to_csv(\"tests_with_predictions_all_models.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa61c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: compute metrics for all models\n",
    "#Load data and merge predictions with ground truth\n",
    "\n",
    "pred = pd.read_csv(\"tests_with_predictions_all_models.csv\")\n",
    "gt = pd.read_csv(\"data-note-with-diseases-label - tests-with-the-ground-truth.csv\")\n",
    "\n",
    "# use the diseases_label column from the ground-truth file\n",
    "df = (\n",
    "    pred.drop(columns=[\"diseases_label\"], errors=\"ignore\")\n",
    "        .merge(gt[[\"_id\", \"diseases_label\"]], on=\"_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"Merged shape:\", df.shape)\n",
    "print(df[[\"_id\", \"diseases_label\"]].head())\n",
    "\n",
    "#Build the label set (10 topics from few-shot sheet)\n",
    "\n",
    "few = pd.read_excel(\"data-note-with-diseases-label.xlsx\", sheet_name=\"few-shot examples\")\n",
    "\n",
    "label_set = sorted({\n",
    "    lab.strip()\n",
    "    for s in few[\"diseases_label\"]\n",
    "    if isinstance(s, str)\n",
    "    for lab in s.split(\",\")\n",
    "})\n",
    "\n",
    "print(\"Evaluation labels:\", label_set)\n",
    "\n",
    "label_index = {lab: i for i, lab in enumerate(label_set)}\n",
    "num_labels = len(label_set)\n",
    "\n",
    "#Helper functions\n",
    "\n",
    "def split_labels_cell(cell):\n",
    "    \"\"\"Turn 'sepsis, pneumonia' into ['sepsis', 'pneumonia'].\"\"\"\n",
    "    if not isinstance(cell, str):\n",
    "        return []\n",
    "    return [x.strip() for x in cell.split(\",\") if x.strip()]\n",
    "\n",
    "\n",
    "def to_multihot(labels):\n",
    "    \"\"\"Convert a list of labels into a multi-hot vector.\"\"\"\n",
    "    v = np.zeros(num_labels, dtype=int)\n",
    "    for lab in labels:\n",
    "        idx = label_index.get(lab)\n",
    "        if idx is not None:\n",
    "            v[idx] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "#ground-truth matrix\n",
    "true_lists = df[\"diseases_label\"].apply(split_labels_cell).tolist()\n",
    "Y_true = np.vstack([to_multihot(lbls) for lbls in true_lists])\n",
    "\n",
    "print(\"Y_true shape:\", Y_true.shape)\n",
    "print(\"Positive count per label:\", dict(zip(label_set, Y_true.sum(axis=0))))\n",
    "\n",
    "# mask: notes that have at least one of the 10 target labels\n",
    "mask_positive_notes = (Y_true.sum(axis=1) > 0)\n",
    "print(\"Number of notes with ≥1 of the 10 labels:\", int(mask_positive_notes.sum()))\n",
    "\n",
    "#Evaluation helpers\n",
    "\n",
    "def evaluate_column(col_name, model_name, mask=None, show_per_label=False):\n",
    "    \"\"\"\n",
    "    Print micro/macro precision/recall/F1 and subset accuracy\n",
    "    for a given prediction column.\n",
    "    mask: optional boolean mask to evaluate on a subset of rows.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"\\n[{model_name}] Column '{col_name}' not found, skipping.\")\n",
    "        return\n",
    "\n",
    "    pred_lists = df[col_name].apply(split_labels_cell).tolist()\n",
    "    Y_pred_full = np.vstack([to_multihot(lbls) for lbls in pred_lists])\n",
    "\n",
    "    if mask is not None:\n",
    "        Y_t = Y_true[mask]\n",
    "        Y_p = Y_pred_full[mask]\n",
    "    else:\n",
    "        Y_t = Y_true\n",
    "        Y_p = Y_pred_full\n",
    "\n",
    "    if Y_t.shape[0] == 0:\n",
    "        print(f\"\\n=== {model_name} ({col_name}) ===\")\n",
    "        print(\"No samples in this scope.\")\n",
    "        return\n",
    "\n",
    "    prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        Y_t, Y_p, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        Y_t, Y_p, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    subset_acc = accuracy_score(Y_t, Y_p)\n",
    "\n",
    "    print(f\"\\n=== {model_name} ({col_name}) ===\")\n",
    "    if mask is None:\n",
    "        print(\"Scope: all test notes\")\n",
    "    else:\n",
    "        print(f\"Scope: notes with ≥1 of the 10 labels (n={int(mask.sum())})\")\n",
    "\n",
    "    print(\"Micro  P/R/F1:\", round(prec_micro, 3), round(rec_micro, 3), round(f1_micro, 3))\n",
    "    print(\"Macro  P/R/F1:\", round(prec_macro, 3), round(rec_macro, 3), round(f1_macro, 3))\n",
    "    print(\"Subset acc   :\", round(subset_acc, 3))\n",
    "\n",
    "    if show_per_label:\n",
    "        prec_lbl, rec_lbl, f1_lbl, sup_lbl = precision_recall_fscore_support(\n",
    "            Y_t, Y_p, average=None, zero_division=0\n",
    "        )\n",
    "        print(\"\\nPer-label metrics:\")\n",
    "        for lab, p, r, f1, sup in zip(label_set, prec_lbl, rec_lbl, f1_lbl, sup_lbl):\n",
    "            print(f\"{lab:30s}  P={p:5.3f}  R={r:5.3f}  F1={f1:5.3f}  support={sup}\")\n",
    "\n",
    "\n",
    "#this version collects results into a summary table\n",
    "summary_rows = []\n",
    "\n",
    "def evaluate_column_to_summary(col_name, model_name, mask=None):\n",
    "    if col_name not in df.columns:\n",
    "        return\n",
    "\n",
    "    pred_lists = df[col_name].apply(split_labels_cell).tolist()\n",
    "    Y_pred_full = np.vstack([to_multihot(lbls) for lbls in pred_lists])\n",
    "\n",
    "    if mask is not None:\n",
    "        Y_t = Y_true[mask]\n",
    "        Y_p = Y_pred_full[mask]\n",
    "        scope = \"positive_only\"\n",
    "    else:\n",
    "        Y_t = Y_true\n",
    "        Y_p = Y_pred_full\n",
    "        scope = \"all_notes\"\n",
    "\n",
    "    if Y_t.shape[0] == 0:\n",
    "        return\n",
    "\n",
    "    prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        Y_t, Y_p, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        Y_t, Y_p, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    subset_acc = accuracy_score(Y_t, Y_p)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"model\": model_name,\n",
    "        \"column\": col_name,\n",
    "        \"scope\": scope,\n",
    "        \"micro_precision\": prec_micro,\n",
    "        \"micro_recall\": rec_micro,\n",
    "        \"micro_f1\": f1_micro,\n",
    "        \"macro_precision\": prec_macro,\n",
    "        \"macro_recall\": rec_macro,\n",
    "        \"macro_f1\": f1_macro,\n",
    "        \"subset_accuracy\": subset_acc,\n",
    "    })\n",
    "\n",
    "\n",
    "#Define models / columns to evaluate\n",
    "\n",
    "models = [\n",
    "    (\"pred_A0_mini\",   \"A0  gpt-4.1-mini v0\"),\n",
    "    (\"pred_A1_mini\",   \"A1  gpt-4.1-mini v1\"),\n",
    "    (\"pred_A2_mini\",   \"A2  gpt-4.1-mini v2\"),\n",
    "    (\"pred_A3_mini\",   \"A3  gpt-4.1-mini v3\"),\n",
    "    (\"pred_B0_gpt41\",  \"B0  gpt-4.1 v0\"),\n",
    "    (\"pred_B1_gpt41\",  \"B1  gpt-4.1 v1\"),\n",
    "    (\"pred_B2_gpt41\",  \"B2  gpt-4.1 v2\"),\n",
    "    (\"pred_B3_gpt41\",  \"B3  gpt-4.1 v3\"),\n",
    "    (\"pred_C0_llama\",  \"C0  LLaMA v0\"),\n",
    "    (\"pred_C1_llama\",  \"C1  LLaMA v1\"),\n",
    "    (\"pred_C2_llama\",  \"C2  LLaMA v2\"),\n",
    "    (\"pred_C3_llama\",  \"C3  LLaMA v3\"),\n",
    "]\n",
    "\n",
    "#Print evaluation for all models\n",
    "\n",
    "\n",
    "print(\"Evaluation on all notes\")\n",
    "\n",
    "for col, name in models:\n",
    "    evaluate_column(col, name, mask=None, show_per_label=False)\n",
    "\n",
    "\n",
    "print(\"Evaluation only on notes with ≥1 target label\")\n",
    "\n",
    "for col, name in models:\n",
    "    evaluate_column(col, name, mask=mask_positive_notes, show_per_label=False)\n",
    "\n",
    "# Example detailed view for one model \n",
    "evaluate_column(\"pred_B1_gpt41\", \"B1  gpt-4.1 v1\", mask=None, show_per_label=True)\n",
    "\n",
    "#Build /save summary table\n",
    "\n",
    "for col, name in models:\n",
    "    evaluate_column_to_summary(col, name, mask=None)\n",
    "    evaluate_column_to_summary(col, name, mask=mask_positive_notes)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\nSummary table:\")\n",
    "print(summary_df)\n",
    "\n",
    "summary_df.to_csv(\"model_evaluation_summary.csv\", index=False)\n",
    "print(\"\\nSaved summary to 'model_evaluation_summary.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
